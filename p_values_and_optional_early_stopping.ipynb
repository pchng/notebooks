{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Avoid Early Stopping in A/B Tests\n",
        "\n",
        "> A while ago, my colleague [YinYin Yu](https://www.linkedin.com/in/yinyin-yu-phd-430951109/) mentioned that early stopping could inflate your false positive rate. This gave me an idea to write a simulation to demonstrate how this could happen, as well as look further into the subject. This notebook is my attempt to document what I've learned.\n",
        "\n",
        "When running an A/B test (experiment), you should outline your testing process _before_ running the experiment, and _not_ change that process in response to what happens during the test. Changing your testing process during the experiment may result in an inflated False Positive Rate (FPR), which is the probability of finding a significant result (rejecting the null hypothesis) when there no true effect. This will make you more often believe there are real results when there are not!\n",
        "\n",
        "One example of the above is **early or optional stopping**. Roughly speaking, this is where you:\n",
        "\n",
        "1. Start the experiment\n",
        "2. As the results come in, you repeatedly check the _p_-value for significance as the experiment progresses. (Also known as _peeking_)\n",
        "3. Based on the result of (2), you decide whether to let the experiment continue:\n",
        "     - a. If the _p_-value is not significant, let the experiment run longer to collect more data.\n",
        "     - b. If the _p_-value **is significant**, stop the experiment and declare a successful outcome.\n",
        "\n",
        "The step in 3(b) is what causes an inflation of the false positive rate, corrupting the results of your experiments and making you find statistically significant results (at a higher rate than expected) when there should not be any."
      ],
      "metadata": {
        "id": "EHS34J9WgT6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Refresher: Hypothesis Testing and the Significance Level\n",
        "\n",
        "An A/B test (e.g. changing a button color) is an example of a _hypothesis test_. In an A/B test, the default assumption (or _null hypothesis_, often designated $H_0$) is that there is no difference between the control and treatment groups being tested. The purpose of a hypothesis test is to conduct an experiment and see whether the null hypothesis (your assumptions) can be rejected or not. (You [can't prove the null hypothesis](https://peterchng.com/blog/2020/05/12/testing-for-no-effect/) with such a test, but that's another story altogether)\n",
        "\n",
        "**When there is no actual difference between the control and treatment groups, _p_-values will have a [uniform distribution between 0 and 1](https://statproofbook.github.io/P/pval-h0)**. ([Another ref](https://towardsdatascience.com/how-to-test-your-hypothesis-using-p-value-uniformity-test-e3a43fc9d1b6))\n",
        "\n",
        "What this means in practical terms:\n",
        "\n",
        "- Even if there is no actual different between control and treatment, you can sometimes get large observed differences between them due to random variation across the population being experimented on.\n",
        "- These large differences result in a low _p_-value, making it seem like there is an actual difference, when there is none.\n",
        "- This can make it seem like there is a true effect when in fact there is none. This is called a _false positive_.\n",
        "\n",
        "How can we control the number of false positives when we run A/B tests?\n",
        "\n",
        "**This where $\\alpha$ (_alpha_), or the significance level, comes in.** The choice of $\\alpha$ controls your false positive rate (FPR) when we have a **decision rule** such as:\n",
        "\n",
        "- **If $p \\leq \\alpha$**: Declare that a true effect exists; reject the null hypothesis and proclaim statistical significance. (e.g. changing the button color really did help improve click-through rate)\n",
        "- **If $p > \\alpha$**: Declare that there is not enough evidence to reject our initial assumption of no difference.\n",
        "\n",
        "If we follow this rule correctly, then no more than $\\alpha$ fraction of statistically significant results will be false positives. This is because under the null hypothesis (no difference between control and treatment groups), the distribution of _p_-values will be uniform between 0 and 1, as previously said. ($\\alpha$ is typically set to $0.05$, but the proper choice of $\\alpha$ is a whole other discussion)\n",
        "\n",
        "If we assume that $\\alpha = 0.05$, this means that **when there is actually no difference between control and treatment**, we will have false positives no more than 5% of the time. This is because under a uniform distribution of _p_-values, no more than 5% will be between $[0, 0.05]$.\n",
        "\n",
        "However, this assumes that the test procedure (in particular, how our decision rule is applied) is followed properly. Let's see what happens when we do not follow the procedure properly."
      ],
      "metadata": {
        "id": "Ij4vEge-imBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of how the False Positive Rate is increased by early stopping using a simulation\n",
        "\n",
        "To see what happens to the number of false positives when we naively do early stopping, let's run a simulation. This simulation will compare the proportion of \"successes\" between two groups - a control group, and a treatment group. This models any A/B test where the response variable is binary, like a click-through rate. **Specifically for each test we will:**\n",
        "\n",
        "1. Draw a number of samples $n_1$ for the control group where each sample has a probability of success of $p_1$. Count the successes as $x_1$.\n",
        "2. Draw an equal number of samples $n_2$ for the treatment group where each sample has a probability of success of $p_2$. Count the successes as $x_2$.\n",
        "3. Compare the proportion of successes in control ($x_1\\over{n_1}$) vs. treatment ($x_2\\over{n_2}$) to determine whether the difference is statistically significant.\n",
        "4. The null hypothesis $H_0$, which is being tested, is that there is no difference between the two groups, i.e. $p_1 = p_2$ or the two groups were sampled from the same population.\n",
        "\n",
        "In our simulation, $p_1$ will be set equal to $p_2$ so that there is indeed no difference between the two groups, and thus we **know** that our null hypothesis $H_0$(assumption of no difference) is true.\n",
        "\n",
        "**We will run many of these tests (each test is steps 1-4 above), and then determine how often we had false positives with and without early stopping.**"
      ],
      "metadata": {
        "id": "cE91C7g8y1tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "It023VBVWQOI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aside: _p_-value calculation\n",
        "The type of hypothesis test we are doing is a two-sample, two-tailed proportions test. We want to compare whether the proportion of successes in one group is statistically different from the proportion of success in another group:\n",
        "- The null hypothesis is that there is no difference.\n",
        "- The test can be done with a [normal approximation](https://www.itl.nist.gov/div898/handbook/prc/section3/prc33.htm) if certain conditions hold (such as a \"large enough\" sample size)\n",
        "- We will be using such a normal approximation instead of an exact test like [Fisher's test](https://en.wikipedia.org/wiki/Fisher%27s_exact_test), which is difficult to calculate for larger sample sizes.\n",
        "- A [Chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test) can be used instead if there are more than two groups or more than two categories of outcomes.\n",
        "- Note that an **approximate test** like this [_does not_ guarantee](https://en.wikipedia.org/wiki/Exact_test) that our false positive rate will be no more than $\\alpha$, our desired Type I error rate. Instead, we only have an approximation that will be more accurate as the sample size increases.\n",
        "\n",
        "To do the test, we need a way to calculate the _p_-value. The function which does that is defined below, along with my explanation of the process."
      ],
      "metadata": {
        "id": "bCVj9k5RWkD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal Approximation to a Binomial Distribution for a two-sample, two-tailed proportions test: [1]\n",
        "# - The p-value from this test answers the question: \"If the two samples were drawn from the same population, how likely is it that we would observe the given difference in proportions?\"\n",
        "# - Various references state that this approximate test can be used when n is \"sufficiently large or p is close to 1/2.\"\n",
        "# - Other guidelines state that it can be used when n*p >= M and n*(p-1) >= M, where M is some number like 5, or 10.\n",
        "# - Strictly speaking the error from this approximation is: (See [2] below)\n",
        "#   a. Minimized when p = 1/2. (All else being equal)\n",
        "#   b. Inversely related to the square root of n.\n",
        "# - For our purposes, the sample sizes we use are probably large enough so as to make the error from this\n",
        "#   approximation negligible for the purposes of this simulation.\n",
        "# References:\n",
        "# 1. https://www.itl.nist.gov/div898/handbook/prc/section3/prc33.htm\n",
        "# 2. https://www.johndcook.com/blog/normal_approx_to_binomial/\n",
        "# 3. https://revisionmaths.com/advanced-level-maths-revision/statistics/normal-approximations\n",
        "\n",
        "# z-score: from two-sample difference of proportions: this uses a normal approximation.\n",
        "# 1. Calculate the difference in proportions as if it was a value from a normal distribution with mean 0.\n",
        "# 2. Then determine how many standard deviations that value is away from 0. This value is the z-score.\n",
        "def z_score(x_1, n_1, x_2, n_2):\n",
        "  # Proportion of successes in group 1 sample\n",
        "  p_hat_1 = x_1/n_1\n",
        "  # Proportion of successes in group 2 sample\n",
        "  p_hat_2 = x_2/n_2\n",
        "\n",
        "  # Pooled proportion of successes\n",
        "  p_hat = (x_1 + x_2)/(n_1 + n_2)\n",
        "\n",
        "  # Estimate of the variance of the test statistic: (p_hat_1 - p_hat_2)\n",
        "  # https://stats.stackexchange.com/questions/361015/proof-of-the-standard-error-of-the-distribution-between-two-normal-distributions/361048\n",
        "  pooled_variance = p_hat * (1 - p_hat) # Estimate of population variance\n",
        "  test_statistic_variance = pooled_variance * (1/n_1 + 1/n_2) # By the properties of variance\n",
        "  test_statistic_standard_error = np.sqrt(test_statistic_variance)\n",
        "\n",
        "  # The z-score is the difference in proportions normalized/divided by the standard error.\n",
        "  z = (p_hat_1 - p_hat_2)/test_statistic_standard_error\n",
        "  return z\n",
        "\n",
        "# p-value is obtained from the z-score.\n",
        "def p_value_z_score(x_1, n_1, x_2, n_2):\n",
        "  z = z_score(x_1, n_1, x_2, n_2)\n",
        "  # Two-tailed test, i.e. consider differences in either direction.\n",
        "  # This uses the survival function (complement of CDF) to determine how much area is under the normal curve from the z-score to infinity.\n",
        "  # It is multiplied by two because we are using a two-tailed test (and the normal distribution is symmetric about 0)\n",
        "  p_value = scipy.stats.norm.sf(abs(z)) * 2\n",
        "  return p_value"
      ],
      "metadata": {
        "id": "ApQcZOJ-YK3G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Definitions\n",
        "n_1 = 1500         # Sample size of group 1\n",
        "n_2 = 1500         # Sample size of group 2\n",
        "p_1 = 0.5          # Probability of success for group 1 samples\n",
        "p_2 = 0.5          # Probability of success for group 2 samples\n",
        "alpha = 0.05       # Significance level; reject H_0 if p-value is <= alpha\n",
        "num_tests = 20000  # Number of tests to run\n",
        "random.seed(1234)  # To get reproducible results; set to None to disable (NOTE: You need to restart the runtime to get reproducible results)"
      ],
      "metadata": {
        "id": "q4Zr-atbZjhW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scenario 1: No early/optional stopping:\n",
        "1. Collect all samples for both groups for the test.\n",
        "2. Calculate the _p_-value **once** per test.\n",
        "3. Use that **single** _p_-value to determine whether to reject the null hypothesis, i.e. do we have statistical significance?\n",
        "4. Count the number of times we (incorrectly) reject the null hypothesis (get a statistically significant result) this way.\n"
      ],
      "metadata": {
        "id": "KBnCCByKaF2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: This simulation could probably be done more efficiently, but is written in a simple/naive way\n",
        "# so that it can be more easily compared with the early stopping example later.\n",
        "\n",
        "num_reject_null = 0 # The number of times we reject the null hypothesis.\n",
        "p_values = [] # Collect all p-values simulated.\n",
        "for test_number in range(num_tests):\n",
        "  x_1 = 0 # Number of successes for group 1\n",
        "  x_2 = 0 # Number of successes for group 2\n",
        "  \n",
        "  # Draw enough samples for group 1 and group 2.\n",
        "  n = max(n_1, n_2)\n",
        "  for sample_num in range(n):\n",
        "    # Any more samples needed for group 1?\n",
        "    if sample_num < n_1:\n",
        "      if random.random() < p_1:\n",
        "        x_1 += 1\n",
        "    # Any more samples needed for group 2?\n",
        "    if sample_num < n_2:\n",
        "      if random.random() < p_2:\n",
        "        x_2 += 1\n",
        "  \n",
        "  p_value = p_value_z_score(x_1=x_1, n_1=n_1, x_2=x_2, n_2=n_2)\n",
        "  p_values.append(p_value)\n",
        "\n",
        "  # Decision rule\n",
        "  if p_value <= alpha:\n",
        "    num_reject_null += 1\n",
        "\n",
        "print('Scenario 1: Results with no early/optional stopping')\n",
        "print(f'We did {num_tests} tests and found statistically significant results {num_reject_null} times')\n",
        "print(f'This is a false positive rate of {num_reject_null/num_tests} which compares with alpha = {alpha}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwkKybjWan0f",
        "outputId": "96148959-2978-4f38-e707-78443032f6f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scenario 1: Results with no early/optional stopping\n",
            "We did 20000 tests and found statistically significant results 1039 times\n",
            "This is a false positive rate of 0.05195 which compares with alpha = 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If we are doing our tests correctly, the fraction of times that we get statistically significant results should be approximately $\\alpha$.** (On account of using an approximate test)\n",
        "\n",
        "This is the purpose of selecting the significance level $\\alpha$ - it is trying to control the number of false positives or _Type I_ errors. If we use the process correctly, the _false positive rate_ (the probability of incorrectly rejecting the null hypothesis when it is actually true) should be no more than the $\\alpha$ level we have set.\n",
        "\n",
        "Note that we can only calculate our false positive rate because we _know_ that the null hypothesis is true here - since we defined $p_1 = p_2$. However in most cases you don't know such parameters - that is why you are running the A/B test!\n",
        "\n",
        "We can also plot a histogram of the _p_-values. It should be _roughly_ uniform between $[0, 1]$."
      ],
      "metadata": {
        "id": "1GFryCnWhCdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: At higher bin counts, the result can look non-uniform. This is because: \n",
        "# 1. The underlying process producing the data is binomial, a discrete distribution.\n",
        "# 2. This causes the range of p-values to be limited to discrete values, making the overall distribution discrete, not continuous.\n",
        "# 3. This causes p-value to \"clump\" together.\n",
        "# 4. At coarse enough bin levels, (e.g. bins=10) it mostly look uniform.\n",
        "# 5. When the number of bins is increased, you will begin to see gaps between the bins, reflecting that some p-values are impossible to obtain under this particular discrete distribution.\n",
        "# For more info, see: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6629378/\n",
        "# - Section: \"The distribution of p values under H 0\"\n",
        "# - Relevant excerpt: \"The p value of Fisher’s exact test is different. Its range is discrete. When the sample size is small, its distribution may be far away from the uniform distribution.\n",
        "#   However, when the sample size is large enough, the p value of the Fisher’s exact is the same as Pearson’s χ2 test. Of course, it is not a wise way to calculate the exact p value in that case.\"\n",
        "counts, bins = np.histogram(p_values, bins=10)\n",
        "plt.stairs(counts, bins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "n2uMtaRWgPQ3",
        "outputId": "e679b8a8-371b-42d6-c735-8a6d06ed1114"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.patches.StepPatch at 0x7fee9ef86fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scenario 2: Early/Optional Stopping: Stop after every $k$ samples and naively check for significance\n",
        "1. We collect up to $n_1$ and $n_2$ samples, and every $k$ samples ($k < n_1, k < n_2$) we stop to calculate the \"current\" _p_-value.\n",
        "2. This means the _p_-value is being calculated **up to** $\\lceil{n\\over{k}}\\rceil$ times for each test where $n = \\max\\{n_1, n_2\\}$\n",
        "3. Each time we calculate a _p_-value, check whether to reject the null hypothesis, e.g. is the _p_-value significant? **If it is significant, then we stop the experiment early and declare a significant result.**\n",
        "4. Count the number of times we (incorrectly) reject the null hypothesis (get a statistically significant result) this way."
      ],
      "metadata": {
        "id": "ywvfYWlPjBfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 100 # Every k samples we will calculate a p-value and check for significance.\n",
        "num_reject_null_2 = 0 # The number of times we reject the null hypothesis.\n",
        "p_values_2 = [] # Collect all p-values simulated.\n",
        "for test_number in range(num_tests):\n",
        "  x_1 = 0 # Number of successes for group 1\n",
        "  x_2 = 0 # Number of successes for group 2\n",
        "  \n",
        "  # Draw enough samples for group 1 and group 2.\n",
        "  n = max(n_1, n_2)\n",
        "  for sample_num in range(n):\n",
        "    # Any more samples needed for group 1?\n",
        "    if sample_num < n_1:\n",
        "      if random.random() < p_1:\n",
        "        x_1 += 1\n",
        "    # Any more samples needed for group 2?\n",
        "    if sample_num < n_2:\n",
        "      if random.random() < p_2:\n",
        "        x_2 += 1\n",
        "  \n",
        "    # Every k samples (or at the end) calculate the p-value.\n",
        "    if sample_num > 0 and (sample_num % k == 0 or sample_num == n - 1):\n",
        "      p_value = p_value_z_score(x_1=x_1, n_1=n_1, x_2=x_2, n_2=n_2)\n",
        "      \n",
        "      # Decision: Does p-value <= alpha? Stop early if so.\n",
        "      if p_value <= alpha:\n",
        "        num_reject_null_2 += 1\n",
        "        break\n",
        "\n",
        "  p_values_2.append(p_value)\n",
        "\n",
        "print('Scenario 2: Results with no early/optional stopping')\n",
        "print(f'We did {num_tests} tests and found statistically significant results {num_reject_null_2} times')\n",
        "print(f'This is a false positive rate of {num_reject_null_2/num_tests} which compares with alpha = {alpha}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6NHzyKJkUxg",
        "outputId": "4b10fb44-ae02-4229-b7a1-5ebb574cc192"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scenario 2: Results with no early/optional stopping\n",
            "We did 20000 tests and found statistically significant results 1981 times\n",
            "This is a false positive rate of 0.09905 which compares with alpha = 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With early/optional stopping the observed false positive rate can be much higher than $\\alpha$**. This means we will think we have statistically significant results much more often than we actually do!\n",
        "\n",
        "This is because we are not using $\\alpha$ properly and can no longer properly control for Type I (false positive) errors.\n",
        "\n",
        "Checking for statistical significance multiple times during the course of the test as you accumulate observation is essentially a form of [multiple comparisons](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). You need to account for this in your testing procedure and not use $\\alpha$ _as if_ you had only computed the _p_-value once at the end of the experiment.\n",
        "\n",
        "The histogram of _p_-values seems to confirm this: We appear to have more lower _p_-values than would be expected from a uniform distribution."
      ],
      "metadata": {
        "id": "VCnlusr7qygc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts_2, bins_2 = np.histogram(p_values_2, bins=10)\n",
        "plt.stairs(counts_2, bins_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "g4tByHIpmeUB",
        "outputId": "3ea129de-adf3-44dc-9e41-c824f2d669d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.patches.StepPatch at 0x7fee9cdbddf0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some lessons from this exercise\n",
        "\n",
        "- The _p_-value **itself** can be seen as a random variable that is uniformly distributed under the null hypothesis.\n",
        "- If you calculate the _p_-value as results come in (looking early, or peeking), it is _somewhat_ like sampling that random variable.\n",
        "- An increased number of looks thus gives you more \"chances\" of getting a statistically significant _p_-value, thus inflating your rate of false positives or Type I errors.\n",
        "\n",
        "The _p_-value will tend to vary the most early on when the number of samples is low. After you have obtained a large amount of samples (assuming the underlying population being sampled is not changing over time), the _p_-value will [tend to \"stabilize/converge\" to the \"true\" _p_-value](https://towardsdatascience.com/how-not-to-run-an-a-b-test-88637a6b921b).\n",
        "\n",
        "However, with a _very_ large number of samples, you may find effect size differences that are too small to be practical. \n",
        "\n",
        "So how do you decide **upfront** when to end your experiment? Ideally you would do a **power analysis** before you start the experiment:\n",
        "\n",
        "1. Decide what is the smallest effect size that would be practically significant, e.g. a 0.1% increase in CTR.\n",
        "2. Decide what statistical power you want, i.e. given an effect size of 0.1%, I want an 80% chance of getting a stat. sig result.\n",
        "3. Decide what your acceptable Type I error rate, i.e. your $\\alpha$ value should be. It might be 5%, or it might be something else, depending on the cost trade-offs between false positives and false negatives.\n",
        "4. From the above three, calculate the minimum number of samples needed to fulfill those criteria using a sample size power calculator.\n",
        "\n",
        "(This is just a high-level summary and the a detailed treatment of power analysis is beyond the scope of this article)\n",
        "\n",
        "Whatever you do, it's important to not modify your test procedure in an ad-hoc manner once the test has started, i.e. decide to collect more samples just because the results \"don't look good\", as such behaviors can inflate error rates."
      ],
      "metadata": {
        "id": "9LUhQeSzTa67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Valid reasons for early stopping\n",
        "\n",
        "In our example simulation above, we decided to peek at results and stop early **only if** the _p_-value was significant. This is not a valid reason, because it biases the outcome toward a significant result, which inflates the false positive rate as we saw. Essentially, we are \"looking\" for a certain result which causes the bias.\n",
        "\n",
        "There are, however, valid reasons to stop a hypothesis test earlier than you initially planned due to unexpected early results. These are usually when you see an unexpectedly large effect size that has practical significance. Some examples:\n",
        "\n",
        "1. **An A/B test is resulting in significant negative metric impact**: If early in your experiment, you see significant negative results (both in a practical and statistical sense), it's probably wise to terminate the experiment early. Something is probably wrong, and terminating the experiment early so you can investigate the cause is prudent.\n",
        "2. **Withholding the potential treatment benefits from the control group would be unethical**: This usually doesn't apply to typical A/B tests, but instead mainly to randomized controlled trials (RCTs) that are assessing the efficacy of some medical or pharmaceutical treatment. These RCTs are also a form of hypothesis test. If testing the efficacy of a potential life-saving treatment, you may find out relatively early on that the benefit is so large and unexpected that it may be considered unethical to continue to withhold that treatment from the control group. (Usually there are [guidelines](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3844122/) around this)\n",
        "\n"
      ],
      "metadata": {
        "id": "ON5SCkuBrND3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to control for early stopping\n",
        "\n",
        "**When the cost of obtaining samples is high, we may want to stop early if certain criteria are met.** Early stopping in and of itself is not necessarily bad. Instead, it's the lack of a proper test design that is the problem. We already saw that using a simple stopping rule like \"_terminate if *p*-value is less than $\\alpha$, and continue if not_\" leads to an inflated false positive rate. Can we use early stopping _and_ also control our false positive rate so it is limited by $\\alpha$?\n",
        "\n",
        "There are some techniques, and this is is the subject of 2021 paper by Lakens et al. entitled [_Group Sequential Designs: A Tutorial_](https://psyarxiv.com/x4azm). The basic idea behind such _Group Sequential Designs_ is that if we are looking at the data multiple times as the results come in, we need to adjust the _p_-value threshold **down** from the original $\\alpha$ level in order to keep the false positive rate under control. The steps are roughly:\n",
        "\n",
        "1. Decide what you want your overall false positive rate (Type I errors) to be bounded by. This is your $\\alpha$ value as before.\n",
        "2. This value of $\\alpha$ can be considered your \"budget\".\n",
        "3. As before, do a power analysis to determine your maximum sample size.\n",
        "4. Decide how you want to spend your \"budget\" each time you want to look early at the results and potentially stop the experiment. \n",
        "5. This requires specifying an _alpha-spending_ function. This function will determine a _p_-value threshold that is _lower_ than your overall $\\alpha$ level. This _p_-value threshold must be used to make a decision on statistical significance each time you look early at interim results.\n",
        "\n",
        "The simplest of these approaches is the [Pocock correction](https://en.wikipedia.org/wiki/Pocock_boundary). Because it is simple, it imposes some restrictions:\n",
        "1. The number of interim looks (stops) must be specified in advance.\n",
        "2. The number of samples collected in between looks must be equal.\n",
        "\n",
        "For example, if we have an overall $\\alpha$ of $0.05$ and we plan to stop five times to look at the results, we have to use an adjusted _p_-value threshold of $0.0158$. This means we can only consider a result statistically significant (and potentially stop early) if $p < 0.0158$. Doing this up to five times will ensure we stay within our overall Type I error rate $\\alpha = 0.05$. (This is conceptually similar to the [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction) for multiple comparisons)\n",
        "\n",
        "For the Pocock correction, the alpha-spending function essentially \"divides\" the original $\\alpha$ value equally. There are more complex alpha-spending functions that _do not_ require you to specify the number nor timing of the interim looks ahead of time. (The section _Stopping for Futility_\" outlines an approach to control Type II errors across multiple looks as well)\n",
        "\n",
        "However, you still need to take care to not make decisions about when to do the interim looks based on the results, since that will introduce bias into the experiment. For example, deciding to do an interim look when the _p_-value \"looks good\" is a recipe for bias.\n",
        "\n",
        "These methods introduce a methodical way to do early stopping, _taking into account_ the impact of the interim looks and using it to adjust the _p_-value threshold accordingly to avoid inflating your false positive rate. The trade-off is a more complex test design. **Hence, such test designs are usually used when the cost of getting more samples is high, and you want to be able to make a decision on early data and have the option to end the test early to save on resources.** This is summarized in a quote from Lakens' paper:\n",
        "\n",
        "> \"Resources in science are limited, and given that group sequential designs can save both time and money required to compensate participants, it is surprising that group sequential designs are not used more widely.\"\n",
        "\n",
        "If the cost of getting more samples is low, it's probably not worthwhile to take on a more complex test design."
      ],
      "metadata": {
        "id": "rEMEomk_rqKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. [How Not to Run an A/B Test](https://towardsdatascience.com/how-not-to-run-an-a-b-test-88637a6b921b)\n",
        "2. [Early stopping in A/B testing](https://bytepawn.com/early-stopping-in-ab-testing.html)\n",
        "3. [Group Sequential Designs: A Tutorial](https://psyarxiv.com/x4azm)\n",
        "4. [How to do A/B test with early stopping correctly](https://www.aarondefazio.com/tangentially/?p=83)\n",
        "5. [When to Adjust Alpha During Multiple Testing: A Consideration of Disjunction, Conjunction, and Individual Testing](https://arxiv.org/ftp/arxiv/papers/2107/2107.02947.pdf)\n",
        "6. [A better stopping rule for conventional statistical tests](https://link.springer.com/content/pdf/10.3758/BF03209488.pdf)\n",
        "7. [Example 111.3 Testing an Effect with Early Stopping to Accept $H_0$](https://documentation.sas.com/doc/en/pgmsascdc/v_023/statug/statug_seqtest_examples03.htm)\n",
        "\n"
      ],
      "metadata": {
        "id": "E0-rM5tRvo07"
      }
    }
  ]
}